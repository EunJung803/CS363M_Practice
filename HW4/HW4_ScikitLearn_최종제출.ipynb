{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBIN LEE\n",
    "# EUNJUNG KIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 : Using `scikit-learn`\n",
    "\n",
    "Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. In this assigment you'll explore how to train various classifiers using the `scikit-learn` library. The scikit-learn documentation can be found [here](http://scikit-learn.org/stable/documentation.html).\n",
    "\n",
    "In this assignment we'll attempt to classify patients as either having or not having diabetic retinopathy, using the same Diabetic Retinopathy data set from your previous assignments. Recall that this dataset contains 1151 records and 20 attributes (some categorical, some continuous). You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may add additional imports\n",
    "import warnings\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>prescreen</th>\n",
       "      <th>ma2</th>\n",
       "      <th>ma3</th>\n",
       "      <th>ma4</th>\n",
       "      <th>ma5</th>\n",
       "      <th>ma6</th>\n",
       "      <th>ma7</th>\n",
       "      <th>exudate8</th>\n",
       "      <th>exudate9</th>\n",
       "      <th>exudate10</th>\n",
       "      <th>exudate11</th>\n",
       "      <th>exudate12</th>\n",
       "      <th>exudate13</th>\n",
       "      <th>exudate14</th>\n",
       "      <th>exudate15</th>\n",
       "      <th>euDist</th>\n",
       "      <th>diameter</th>\n",
       "      <th>amfm_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.895756</td>\n",
       "      <td>17.775994</td>\n",
       "      <td>5.270920</td>\n",
       "      <td>0.771761</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>57.709936</td>\n",
       "      <td>23.799994</td>\n",
       "      <td>3.325423</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>55.831441</td>\n",
       "      <td>27.993933</td>\n",
       "      <td>12.687485</td>\n",
       "      <td>4.852282</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>0.128548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>40.467228</td>\n",
       "      <td>18.445954</td>\n",
       "      <td>9.118901</td>\n",
       "      <td>3.079428</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>18.026254</td>\n",
       "      <td>8.570709</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475935</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>28.356400</td>\n",
       "      <td>6.935636</td>\n",
       "      <td>2.305771</td>\n",
       "      <td>0.323724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502831</td>\n",
       "      <td>0.126741</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>15.448398</td>\n",
       "      <td>9.113819</td>\n",
       "      <td>1.633493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541743</td>\n",
       "      <td>0.139575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.679649</td>\n",
       "      <td>9.497786</td>\n",
       "      <td>1.223660</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576318</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>66.691933</td>\n",
       "      <td>23.545543</td>\n",
       "      <td>6.151117</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500073</td>\n",
       "      <td>0.116793</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>22.141784</td>\n",
       "      <td>10.054384</td>\n",
       "      <td>0.874633</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560959</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
       "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
       "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
       "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
       "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
       "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
       "5        1          1   44   43   41   41   37   29  28.356400   6.935636   \n",
       "6        1          0   29   29   29   27   25   16  15.448398   9.113819   \n",
       "7        1          1    6    6    6    6    2    1  20.679649   9.497786   \n",
       "8        1          1   22   21   18   15   13   10  66.691933  23.545543   \n",
       "9        1          1   79   75   73   71   64   47  22.141784  10.054384   \n",
       "\n",
       "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
       "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
       "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
       "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
       "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
       "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
       "5   2.305771   0.323724   0.000000   0.000000   0.000000   0.000000  0.502831   \n",
       "6   1.633493   0.000000   0.000000   0.000000   0.000000   0.000000  0.541743   \n",
       "7   1.223660   0.150382   0.000000   0.000000   0.000000   0.000000  0.576318   \n",
       "8   6.151117   0.496372   0.000000   0.000000   0.000000   0.000000  0.500073   \n",
       "9   0.874633   0.099780   0.023386   0.000000   0.000000   0.000000  0.560959   \n",
       "\n",
       "   diameter  amfm_class  label  \n",
       "0  0.100025           1      0  \n",
       "1  0.144414           0      0  \n",
       "2  0.128548           0      1  \n",
       "3  0.114790           0      0  \n",
       "4  0.123572           0      1  \n",
       "5  0.126741           0      1  \n",
       "6  0.139575           0      1  \n",
       "7  0.071071           1      0  \n",
       "8  0.116793           0      1  \n",
       "9  0.109134           0      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from csv file\n",
    "col_names = []\n",
    "for i in range(20):\n",
    "    if i == 0:\n",
    "        col_names.append('quality')\n",
    "    if i == 1:\n",
    "        col_names.append('prescreen')\n",
    "    if i >= 2 and i <= 7:\n",
    "        col_names.append('ma' + str(i))\n",
    "    if i >= 8 and i <= 15:\n",
    "        col_names.append('exudate' + str(i))\n",
    "    if i == 16:\n",
    "        col_names.append('euDist')\n",
    "    if i == 17:\n",
    "        col_names.append('diameter')\n",
    "    if i == 18:\n",
    "        col_names.append('amfm_class')\n",
    "    if i == 19:\n",
    "        col_names.append('label')\n",
    "\n",
    "data = pd.read_csv(\"messidor_features.txt\", names = col_names)\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. All of the classifiers in `scikit-learn` require that you separate the feature columns from the class label column, so go ahead and do that first. You should end up with two separate data frames: one that contains all of the feature values and one that contains the class labels. \n",
    "\n",
    "Note: Later in this assignment, you may get a warning stating \"a column-vector was passed when a 1d array was expected.\" This indicates that some function wants a _flat array_ of labels, rather than a 2D DataFrame of labels. You can go ahead and transform the labels into a flat array here by doing either `labels.values.ravel()` or `labels.iloc[:,0]`. And you can just use that flat array for everything.\n",
    "\n",
    "Print the `shape` of your features data frame, the shape or len of your labels dataframe or array, and the `head` of the features data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 19) (1150, 1)\n",
      "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
      "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
      "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
      "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
      "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
      "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
      "\n",
      "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
      "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
      "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
      "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
      "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
      "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
      "\n",
      "   diameter  amfm_class  \n",
      "0  0.100025           1  \n",
      "1  0.144414           0  \n",
      "2  0.128548           0  \n",
      "3  0.114790           0  \n",
      "4  0.123572           0  \n"
     ]
    }
   ],
   "source": [
    "y = data.iloc[:,[-1]]\n",
    "X = data.drop(y.columns,axis = 1)\n",
    "print(X.shape, y.shape)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Decision Trees (DT) & Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. You can train a classifier using the holdout method by splitting your data into a  training set and a  test set, then you can evaluate the classifier on the held-out test set. \n",
    "\n",
    "Let's try this with a decision tree classifier. \n",
    "\n",
    "* Use `sklearn.model_selection.train_test_split` to split your dataset into training and test sets (do an 80%-20% split). Display how many records are in the training set and how many are in the test set.\n",
    "* Use `sklearn.tree.DecisionTreeClassifier` to fit a decision tree classifier on the training set. Use entropy as the split criterion. \n",
    "* Now that the tree has been learned from the training data, we can run the test data through and predict classes for the test data. Use the `predict` method of `DecisionTreeClassifier` to classify the test data. \n",
    "* Then use `sklearn.metrics.accuracy_score` to print out the accuracy of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train : 920 X test : 230\n",
      "0.6478260869565218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X,y,test_size=0.2)\n",
    "print('X train :',len(X_train),'X test :',len(X_test))\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "score = sk.metrics.accuracy_score(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Note that the DecisionTree classifier has many parameters that can be set. Try tweaking parameters like split criterion, max_depth, min_impurity_decrease, min_samples_leaf, min_samples_split, etc. to see how they affect accuracy. Print the accuracy of a few different variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini criterion:  0.5652173913043478\n",
      "Gini criterion & max_depth = 30 :  0.5608695652173913\n",
      "Gini criterion & max_depth = 30 & min_samples_split = 3:  0.5521739130434783\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion='gini')\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "score = sk.metrics.accuracy_score(y_test,y_pred)\n",
    "print('Gini criterion: ',score)\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth = 30)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "score = sk.metrics.accuracy_score(y_test,y_pred)\n",
    "print('Gini criterion & max_depth = 30 : ',score)\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth = 30, min_samples_split=10)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "score = sk.metrics.accuracy_score(y_test,y_pred)\n",
    "print('Gini criterion & max_depth = 30 & min_samples_split = 3: ',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. You have now built a decision tree and tested it's accuracy using the \"holdout\" method. But as discussed in class, this is not sufficient for estimating generalization accuracy. Instead, we should use Cross Validation to get a better estimate of accuracy. \n",
    "\n",
    "Use `sklearn.model_selection.cross_val_score` to perform 10-fold cross validation on a decision tree. You will pass the FULL dataset into `cross_val_score` which will automatically divide it into the number of folds you tell it to, train a decision tree model on the training set for each fold, and test it on the test set for each fold. It will return a numpy array with the accuracy out of each fold. Average these accuracies to print out the generalization accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6104347826086955"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "acc = cross_val_score(dt,X,y,cv=10)\n",
    "acc = np.mean(acc)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nested Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Now we want to tune our model to use the best parameters to avoid overfitting to our training data. Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters (hyperparameters) specified in a grid. \n",
    "* Use `sklearn.model_selection.GridSearchCV` to find the best `max_depth`, `max_features`, and `min_samples_leaf` for your tree. Use a 5-fold-CV and 'accuracy' for the scoring criteria.\n",
    "* Try the values [5,10,15,20] for `max_depth` and `min_samples_leaf`. Try [5,10,15] for `max_features`. \n",
    "* Print out the best value for each of the tested parameters (`best_params_`).\n",
    "* Print out the accuracy of the model with these best values (`best_score_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 20, 'max_features': 10, 'min_samples_leaf': 10}\n",
      "Best score:  0.6660869565217391\n"
     ]
    }
   ],
   "source": [
    "params=[{'max_depth':[5,10,15,20], 'min_samples_leaf':[5,10,15,20], 'max_features':[5,10,15]}]\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "gs = sk.model_selection.GridSearchCV(estimator=dt,param_grid=params,scoring='accuracy',cv=5)\n",
    "gs.fit(X,y)\n",
    "print(\"Best parameters: \", gs.best_params_)\n",
    "print(\"Best score: \", gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What you did in Q5 performed the _inner_ loop of a nested CV (no test set was held out). What you did in Q4 performed an _outer_ loop of CV (holds out a test set). Now we need to combine them to perform the nested cross-validation that we discussed in class. To do this, you'll need to pass the a `GridSearchCV` into a `cross_val_score`. \n",
    "\n",
    "What this does is: the `cross_val_score` splits the data in to train and test sets for the first outer fold, and it passes the train set into `GridSearchCV`. `GridSearchCV` then splits that set into train and validation sets for k number of folds (the inner CV loop). The hyper-parameters for which the average score over all inner iterations is best, is reported as the `best_params_`, `best_score_`, and `best_estimator_`(best decision tree). This best decision tree is then evaluated with the test set from the `cross_val_score` (the outer CV loop). And this whole thing is repeated for the remaining k folds of the `cross_val_score` (the outer CV loop). \n",
    "\n",
    "That is a lot of explanation for a very complex (but IMPORTANT) process, which can all be performed with a single line of code!\n",
    "\n",
    "Be patient for this one to run. The nested cross-validation loop can take some time. A [ * ] next to the cell indicates that it is still running.\n",
    "\n",
    "Print the accuracy of your tuned, cross-validated model. This is the official accuracy that you would report for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score:  0.6426086956521739\n",
      "Best parameters:  {'max_depth': 20, 'max_features': 15, 'min_samples_leaf': 5}\n",
      "Best score:  0.6478260869565217\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "params=[{'max_depth':[5,10,15,20], 'min_samples_leaf':[5,10,15,20], 'max_features':[5,10,15]}]\n",
    "\n",
    "gs = sk.model_selection.GridSearchCV(estimator=clf,param_grid=params,scoring='accuracy',cv=5)\n",
    "gs.fit(X,y)\n",
    "scores = cross_val_score(gs, X, y, cv=5)\n",
    "\n",
    "\n",
    "print(\"Average score: \", scores.mean())\n",
    "print(\"Best parameters: \", gs.best_params_)\n",
    "print(\"Best score: \", gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Naive Bayes (NB) & Evaluation Metrics\n",
    "\n",
    "`sklearn.naive_bayes.GaussianNB` implements the Gaussian Naive Bayes algorithm for classification. This means that the liklihood of continuous features is estimated using a Gaussian distribution. (Refer to slide 13 of the Naive Bayes powerpoint notes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Create a `sklearn.naive_bayes.GaussianNB` classifier. Use `sklearn.model_selection.cross_val_score` to do a 10-fold cross validation on the classifier. Display the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5956521739130435"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "acc = cross_val_score(nb,X_train,y_train.values.ravel(),cv=10)\n",
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. `cross_val_score` returns the scores of every test fold. There is another function called `cross_val_predict` that returns predicted y values for every record in the test fold. In other words, for each element in the input, `cross_val_predict` returns the prediction that was obtained for that element when it was in the test set. \n",
    "\n",
    "* Use `cross_val_predict` and `sklearn.metrics.confusion_matrix` to print the confusion matrix for the classifier.\n",
    "\n",
    "* Sckit-learn also provides a useful function `sklearn.metrics.classification_report` for evaluating the classifier on a per-class basis. It is a summary of the precision, recall, and F1 score for each class (and support is just the actual class count). Display the classification report for your Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 1</th>\n",
       "      <th>Predicted 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>96</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>77</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 1  Predicted 0\n",
       "Actual 1           96           12\n",
       "Actual 0           77           45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.89      0.68       108\n",
      "           1       0.79      0.37      0.50       122\n",
      "\n",
      "    accuracy                           0.61       230\n",
      "   macro avg       0.67      0.63      0.59       230\n",
      "weighted avg       0.68      0.61      0.59       230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(table_data):\n",
    "    df = pd.DataFrame(table_data, columns =['Predicted 1','Predicted 0'])\n",
    "    df = df.rename(index={0: 'Actual 1', 1: 'Actual 0'})\n",
    "    display(df)\n",
    "    \n",
    "y_pred = cross_val_predict(nb,X_test, y_test.values.ravel(), cv=10)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print_confusion_matrix(cm)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Using `sklearn.metrics.roc_curve` plot a ROC curve for the Naive Bayes classifier. Also calculate the area under the curve (AUC) using `sklearn.metrics.roc_auc_score`.\n",
    "\n",
    "* We will just do this on a single holdout test set (because it gets more complicated to put this inside of a cross-validation). So, split your data into training and test sets using `sklearn.model_selection.train_test_split`. Do an 80/20 split.\n",
    "* Fit the Naive Bayes classifier to the training data by calling the `fit` method on the trainng data.\n",
    "* Now call the `predict_proba` method on your classifier and pass in the test data. This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the probability that this datapoint is in class 1.\n",
    "* We are going to say that class 1 (having the disease) is the rare/positive class. To create a ROC curve, pass the actual Y labels and the probabilites of class 1 (column index 1 out of your predict_proba result) into `sklearn.metrics.roc_curve`\n",
    "* Pass the FPR and TPR that `roc_curve` returns into the plotting code that we have provided you.\n",
    "* Print the AUC (area under the curve) by using `sklearn.metrics.roc_auc_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soobin/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict_proba(X_test)[:,1]\n",
    "\n",
    "#replace these fpr and tpr with the results of your roc_curve\n",
    "# fpr, tpr = [], []\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Do not change this code! This plots the ROC curve.\n",
    "# Just replace the fpr and tpr above with the values from your roc_curve\n",
    "plt.plot([0,1],[0,1],'k--') #plot the diagonal line\n",
    "plt.plot(fpr, tpr, label='NB') #plot the ROC curve\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('ROC Curve Naive Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. k-Nearest Neighbor (KNN) & Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some classification algorithms, scaling of the data is critical (like KNN, SVM, Neural Nets). For other classification algorithms, data scaling is not necessary (like Naive Bayes and Decision Trees). _Take a minute to think about why this is the case!!_ But using scaled data with an algorithm that doesn't explicitly need it to be scaled does not hurt the results of that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. The distance calculation method is central to the KNN algorithm. By default, `KNeighborsClassifier` uses  Euclidean distance as its metric (but this can be changed). Because of the distance calculations, it is critical to scale the data before running Nearest Neighbor!\n",
    "\n",
    "We discussed why dimensionality reduction may also be needed with KNN because of the curse of dimensionality. So we may want to also perform a dimensionality reduction with PCA before running KNN. PCA should only be performed on scaled data! (Remember that you can also reduce dimensionality by performing feature selection and feature engineering.) \n",
    "\n",
    "An important note about scaling data and dimensionality reduction is that they should only be performed on the **training** data, then you transform the test data into the scaled, PCA space that was found on the training data. (Refer to the concept of [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/).)\n",
    "\n",
    "So when you are doing cross-validation, the scaling and PCA needs to happen *inside of your CV loop*. This way, it is performed on the training set for the first fold, then the test set is put into that space. On the second fold, it is performed on the trainng set for the second fold, and the test set is put into that space. And so on for the remaining folds. \n",
    "\n",
    "In order to do this with scikit-learn, you must create what's called a `Pipeline` and pass that in to the cross validation. This is a very important concept for Data Mining and Machine Learning, so let's practice it here.\n",
    "\n",
    "Do the following:\n",
    "* Create a `sklearn.preprocessing.StandardScaler` object to standardize the dataset’s features (mean = 0 and variance = 1). (Do not call `fit` on it yet. Just create the `StandardScaler` object.)\n",
    "* Create a `sklearn.decomposition.PCA` object to perform PCA dimensionality reduction. (Do not call `fit` on it yet. Just create the `PCA` object.)\n",
    "* Create a `sklearn.neighbors.KNeighborsClassifier`. The number of neighbors defaults to 5 (k=5). Go ahead and change it to 7. (Do not call `fit` on it yet. Just create the `KNeighborsClassifier` object.)\n",
    "* Create a `sklearn.pipeline.Pipeline` object and set the `steps` to the scaler, the PCA, and the KNN objects that you just created. \n",
    "* Pass the `pipeline` object in to a `cross_val_score` as the estimator, along with the features and the labels, and use a 5-fold-CV. \n",
    "\n",
    "In each fold of the cross validation, the training phase will use _only_ the training data for scaling, PCA, and training the model. Then the testing phase will scale & transform the test data into the PCA space (found on the training data) and run the test data through the trained classifier, to return an accuracy measurement for each fold. Print the average accuracy across all 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.6326086956521739\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Average accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Another important part of KNN is choosing the best number of neighbors (tuning the hyperparameter, k). We can use nested cross validation to do this. Let's try k values from 1-25 to find the best one. \n",
    "\n",
    "We _also_ want to find the best number of dimensions to project down onto using PCA. We can use nested cross validation to do this as well. Let's try from 5-19 dimensions.\n",
    "\n",
    "* Starter code is provided to create the \"parameter grid\" to search. You will need to change this code! Where I have \"knn__n_neighbors\", this indicates that I want to tune the \"n_neighbors\" parameter in the \"knn\" part of the pipeline. When you created your pipeline above, you named the KNN part of the pipeline with a string. You should replace \"knn\" in the param_grid below with whatever you named your KNN part of the pipeline: **<replace_this>__n_neighbors.** Do the same for the PCA part of the pipeline.\n",
    "* Create a `sklearn.model_selection.GridSearchCV` and pass in the pipeline, the param_grid, and set it to a 5-fold-CV.\n",
    "* Now, on that `GridSearchCV` object, call `fit` and pass in the features and labels.\n",
    "* Show the best number of dimensions and best number of neighbors for this dataset by printing the `best_params_` from the `GridSearchCV`.\n",
    "* Also print the accuracy when using this best number of dimensions and neighbors by printing the `best_score_` from the `GridSearchCV`.\n",
    "\n",
    "Be patient, this can take some time to run. It is trying every combination of dimensions from 5-19 with every k from 1-25! A [ * ] next to the cell indicates that it is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of dimensions: 14\n",
      "Best number of neighbors: 23\n",
      "Best accuracy: 0.6617391304347826\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "On the \"pca\" part of the pipeline, \n",
    "tune the n_components parameter,\n",
    "by trying the values 1-19.\n",
    "\n",
    "On the \"knn\" part of the pipeline, \n",
    "tune the n_neighbors parameter,\n",
    "by trying the values 1-25.\n",
    "'''\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(5, 19)),\n",
    "    'knn__n_neighbors': list(range(1, 25))\n",
    "}\n",
    "\n",
    "# your code goes here\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# print(\"Best accuracy:\", grid_search.best_score_)\n",
    "\n",
    "print(\"Best number of dimensions:\", grid_search.best_params_['pca__n_components'])\n",
    "print(\"Best number of neighbors:\", grid_search.best_params_['knn__n_neighbors'])\n",
    "print(\"Best accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. In Q11, we did not hold out a test set. The accuracy reported out is on the _validation_ set. So now we need to wrap the whole process in another cross-validation to perform a nested cross-validation and report the accuarcy of this KNN model on unseen test data. This is the official accuracy you would report on this model.\n",
    "\n",
    "You'll need to pass the `GridSearchCV` into a `cross_val_score`, just as you did with the decision tree. Use a 5-fold-CV for the outer loop. \n",
    "\n",
    "Again, be patient for this one to run. The nested cross-validation loop can take some time. It is doing what it did above in Q11 five times. A [ * ] next to the cell indicates that it is still running. (Just for comparison, mine takes about 2 mins to run and the fan revs up so it sounds like my computer is going to explode. All computers are different, so yours could take shorter or longer...)\n",
    "\n",
    "<img src=\"model_is_training.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested CV average score: 0.5957\n"
     ]
    }
   ],
   "source": [
    "nested_cv_scores = cross_val_score(GridSearchCV(pipe, param_grid, cv=5), X_test, y_test, cv=5)\n",
    "\n",
    "print(\"Nested CV average score: %.4f\" % np.mean(nested_cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13. Now put it all together with an SVM. \n",
    "* Create a `pipeline` that includes scaling, PCA, and an `sklearn.svm.SVC`.\n",
    "* Create a parameter grid that tries number of dimensions from 5-19 and SVM kernels `linear`, `rbf` and `poly`.\n",
    "* Create a `GridSearchCV` for the inner CV loop. Use a 5-fold CV.\n",
    "* Run a `cross_val_predict` with a 10-fold CV for the outer loop. \n",
    "* Print out the accuracy and the classification report of using an SVM classifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7239130434782609\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.83      0.74       436\n",
      "           1       0.81      0.62      0.70       484\n",
      "\n",
      "    accuracy                           0.72       920\n",
      "   macro avg       0.74      0.73      0.72       920\n",
      "weighted avg       0.74      0.72      0.72       920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "param_grid_svm = {\n",
    "    'pca__n_components': list(range(5, 19)),\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(pipe_svm, param_grid_svm, cv=5)\n",
    "\n",
    "y_pred_svm = cross_val_predict(grid_search_svm, X_train, y_train, cv=10)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred_svm))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_train, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. Train a multi-layer perceptron with a single hidden layer using `sklearn.neural_network.MLPClassifier`. \n",
    "* Create a pipeline with scaling and a neural net. (No PCA on this one. But scaling is critical to neural nets.)\n",
    "* Use `GridSearchCV` with 5 fold cross validation to find the best hidden layer size and the best activation function. \n",
    "* Try values of `hidden_layer_sizes` ranging from `(30,)` to `(60,)` by increments of 10.\n",
    "* Try activation functions `logistic`, `tanh`, `relu`.\n",
    "* Wrap your `GridSearchCV` in a 5-fold `cross_val_score` and report the accuracy of your neural net.\n",
    "\n",
    "Be patient, as this can take a few minutes to run. You may get ConvergenceWarnings as it runs - that is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7097826086956521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pipe_nn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MLPClassifier())\n",
    "])\n",
    "\n",
    "param_grid_nn = {\n",
    "    'nn__hidden_layer_sizes': [(i,) for i in range(30, 61, 10)],\n",
    "    'nn__activation': ['logistic', 'tanh', 'relu']\n",
    "}\n",
    "\n",
    "grid_search_nn = GridSearchCV(pipe_nn, param_grid_nn, cv=5)\n",
    "\n",
    "scores = cross_val_score(grid_search_nn, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Ensemble Classifiers\n",
    "\n",
    "Ensemble classifiers combine the predictions of multiple base estimators to improve the accuracy of the predictions. One of the key assumptions that ensemble classifiers make is that the base estimators are built independently (so they are diverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forests**\n",
    "\n",
    "Q15. Use `sklearn.ensemble.RandomForestClassifier` to classify the data. Scaling the data is not necessary for Decision Trees (take a minute to think about why). So, no need for a pipeline here.\n",
    "\n",
    "The default for RandomForest is to use 100 fully-grown decision trees. Let's use a `GridSearchCV` with a 5-fold CV to try various numbers of base classifiers and select the one with the best results. \n",
    "\n",
    "Try `n_estimators` of 50, 100, and 150 - this is the number of base classifiers in the ensemble. Wrap your GridSearchCV in a cross_val_score with 5-fold CV. Display the classification report. \n",
    "\n",
    "Note that this does get a higher accuracy than a single decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'n_estimators': 50}\n",
      "Accuracy score:  0.6891304347826086\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71       103\n",
      "           1       0.78      0.69      0.73       127\n",
      "\n",
      "    accuracy                           0.72       230\n",
      "   macro avg       0.72      0.72      0.72       230\n",
      "weighted avg       0.72      0.72      0.72       230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc_cv = GridSearchCV(rfc, param_grid, cv=5)\n",
    "\n",
    "rfc_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: \", rfc_cv.best_params_)\n",
    "print(\"Accuracy score: \", rfc_cv.best_score_)\n",
    "\n",
    "rfc_cv_scores = cross_val_score(rfc_cv.best_estimator_, X_test, y_test, cv=5)\n",
    "\n",
    "print(\"Classification report:\\n\", classification_report(y_test, rfc_cv.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost**\n",
    "\n",
    "Random Forests builds its base estimators independently, using bagging. There is another method of training ensemble classifiers called boosting. Here the classifiers are trained sequentially and each time the sampling of the training set depends on the performance of previously generated models.\n",
    "\n",
    "Q16. Evaluate a `sklearn.ensemble.AdaBoostClassifier` classifier on the data. By default, `AdaBoostClassifier` uses 50 decision stumps as the base classifiers. Let's again use a `GridSearchCV` with a 5-fold CV to try various numbers of base classifiers.\n",
    "\n",
    "Try `n_estimators` of 50, 100, and 150 - this is the number of base classifiers in the ensemble. Wrap your GridSearchCV in a cross_val_score with 5-fold CV. Display the classification report.\n",
    "\n",
    "Note that even when using decision stumps as the base classifier, this gets a higher accuracy than a single decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6902 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "# define the classifier and parameter grid\n",
    "ada_boost = AdaBoostClassifier()\n",
    "param_grid = {'n_estimators': [50, 100, 150]}\n",
    "\n",
    "# perform grid search with 5-fold CV\n",
    "grid_search = GridSearchCV(ada_boost, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train,y_train)\n",
    "scores = cross_val_score(grid_search, X_train, y_train, cv=5)\n",
    "\n",
    "# print the classification report\n",
    "print('Accuracy: {:.4f} '.format(scores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Build your final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have tested all kinds of classifiers on this data. Some have performed better than others. \n",
    "\n",
    "Q17. We may not want to deploy any of these models in the real world to actually diagnose patients because the accuracies are not high enough. What can we do to improve the accuracy rates? Answer as a comment:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Collect more data to increase the size of the training set\n",
    "\n",
    "2. Acquire additional pertinent features that may suggest the presence of the disease.\n",
    "\n",
    "3. To improve the accuracy of our models, we can try to select the most informative features and only use those in them.\n",
    "\n",
    "4. Fine-tune the models' hyperparameters to optimize their performance.\n",
    "\n",
    "5. Explore ensembling or stacking various models to utilize their unique capabilities and possibly enhance the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q18. Let's say we *did* get to the point where we had a model with high enough accuracy and we want to deploy that model and use it for real-world predictions.\n",
    "\n",
    "* Let's say we're going to deploy our SVM classifier.\n",
    "* We need to make one final version of this model, where we use ALL of our available data for training (we do not hold out a test set this time, so no outer cross-validation loop). \n",
    "* We need to tune the parameters of the model on the FULL dataset, so copy the code you entered for Q13, but remove the outer cross validation loop (remove `cross_val_score`). Just run the `GridSearchCV` by calling `fit` on it and passing in the full dataset. This results in the final trained model with the best parameters for the full dataset. You can print out `best_params_` to see what they are.\n",
    "* The accuracy of this model is what you assessed and reported in Q13.\n",
    "\n",
    "\n",
    "* Use the `pickle` package to save your model. We have provided the lines of code for you, just make sure your final model gets passed in to `pickle.dump()`. This will save your model to a file called finalized_model.sav in your current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'pca__n_components': 18, 'svm__kernel': 'linear'}\n",
      "Accuracy score:  0.7208695652173913\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "param_grid_svm = {\n",
    "    'pca__n_components': list(range(5, 19)),\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(pipe_svm, param_grid_svm, cv=5)\n",
    "grid_search_svm.fit(X, y)\n",
    "\n",
    "print(\"Best parameters: \", grid_search_svm.best_params_)\n",
    "print(\"Accuracy score: \", grid_search_svm.best_score_)\n",
    "\n",
    "#replace this final_model with your final model\n",
    "final_model = grid_search_svm.best_estimator_\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q19. Now if someone wants to use your trained, saved classifier to classify a new record, they can load the saved model and just call `predict` on it. \n",
    "* Given this new record, classify it with your saved model and print out either \"Negative for disease\" or \"Positive for disease.\"\n",
    "* Note that `predict` is expecting a list of lists (a list of records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive for disease\n"
     ]
    }
   ],
   "source": [
    "# some time later...\n",
    "\n",
    "# use this as the new record to classify\n",
    "record = [ 0.05905386, 0.2982129, 0.68613149, 0.75078865, 0.87119216, 0.88615694,\n",
    "  0.93600623, 0.98369184, -0.47426472, -0.57642756, -0.53115361, -0.42789774,\n",
    " -0.21907738, -0.20090532, -0.21496782, -0.2080998, 0.06692373, -2.81681183,\n",
    " -0.7117194 ]\n",
    "\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "prediction = loaded_model.predict([record])\n",
    "\n",
    "if prediction == 0:\n",
    "    print(\"Negative for disease\")\n",
    "else:\n",
    "    print(\"Positive for disease\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
